{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLCAjoOBShOxn3KBp/3QWh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharunakula/TensorFlow-Exercises/blob/main/1.Tensor%20Manipulations%20%26%20Reshaping\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vkp1qfY2NRz",
        "outputId": "28196c9e-4f8f-46af-affb-d916c0dc10a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tensor:\n",
            "tf.Tensor(\n",
            "[[-1.5628458   0.50304425 -0.1155692  -0.11570568  0.4351735  -1.0727228 ]\n",
            " [ 1.4864957  -1.9571458   0.08340666  1.3531728   0.04859374  0.6733272 ]\n",
            " [-0.5800335   0.54201895 -0.96904266 -0.93309957  0.9663639   0.18504436]\n",
            " [ 1.5875164  -0.13007957  1.2621782  -0.7089399   1.1843717   0.19293486]], shape=(4, 6), dtype=float32)\n",
            "Rank before reshaping: 2\n",
            "Shape before reshaping: [4 6]\n",
            "\n",
            "Reshaped Tensor:\n",
            "tf.Tensor(\n",
            "[[[-1.5628458   0.50304425 -0.1155692  -0.11570568]\n",
            "  [ 0.4351735  -1.0727228   1.4864957  -1.9571458 ]\n",
            "  [ 0.08340666  1.3531728   0.04859374  0.6733272 ]]\n",
            "\n",
            " [[-0.5800335   0.54201895 -0.96904266 -0.93309957]\n",
            "  [ 0.9663639   0.18504436  1.5875164  -0.13007957]\n",
            "  [ 1.2621782  -0.7089399   1.1843717   0.19293486]]], shape=(2, 3, 4), dtype=float32)\n",
            "Rank after reshaping: 3\n",
            "Shape after reshaping: [2 3 4]\n",
            "\n",
            "Transposed Tensor:\n",
            "tf.Tensor(\n",
            "[[[-1.5628458   0.50304425 -0.1155692  -0.11570568]\n",
            "  [-0.5800335   0.54201895 -0.96904266 -0.93309957]]\n",
            "\n",
            " [[ 0.4351735  -1.0727228   1.4864957  -1.9571458 ]\n",
            "  [ 0.9663639   0.18504436  1.5875164  -0.13007957]]\n",
            "\n",
            " [[ 0.08340666  1.3531728   0.04859374  0.6733272 ]\n",
            "  [ 1.2621782  -0.7089399   1.1843717   0.19293486]]], shape=(3, 2, 4), dtype=float32)\n",
            "Rank after transposing: 3\n",
            "Shape after transposing: [3 2 4]\n",
            "\n",
            "Result after broadcasting and addition:\n",
            "tf.Tensor(\n",
            "[[[-0.5338875  -0.48520756 -0.5178503   0.64159805]\n",
            "  [ 0.44892484 -0.44623286 -1.3713237  -0.17579585]]\n",
            "\n",
            " [[ 1.4641318  -2.0609746   1.0842147  -1.1998421 ]\n",
            "  [ 1.9953222  -0.80320746  1.1852354   0.62722415]]\n",
            "\n",
            " [[ 1.112365    0.36492097 -0.35368735  1.4306309 ]\n",
            "  [ 2.2911365  -1.6971917   0.78209066  0.9502386 ]]], shape=(3, 2, 4), dtype=float32)\n",
            "\n",
            "Explanation of Broadcasting:\n",
            "\n",
            "Broadcasting in TensorFlow (and NumPy) allows arithmetic operations between tensors of different shapes, \n",
            "as long as certain compatibility rules are met.  It avoids explicit data replication, making operations \n",
            "more memory and computationally efficient.\n",
            "\n",
            "In this example, 'smaller_tensor' with shape (1, 4) is added to 'transposed_tensor' with shape (3, 2, 4).  \n",
            "Broadcasting works as follows:\n",
            "\n",
            "1. **Dimension Alignment:** TensorFlow compares the shapes of the two tensors dimension by dimension, starting from the trailing dimensions (rightmost).\n",
            "\n",
            "2. **Compatibility Rules:** Two dimensions are compatible if:\n",
            "   a) They are equal, or\n",
            "   b) One of them is 1.\n",
            "\n",
            "3. **Expansion:**  If a dimension in one tensor is 1, TensorFlow \"stretches\" or \"copies\" that dimension to match the corresponding dimension in the other tensor.  This is the \"broadcasting\" part.\n",
            "\n",
            "In our case:\n",
            "- (3, 2, 4) and (1, 4)\n",
            "\n",
            "- The last dimension (4) matches.\n",
            "- The second to last dimension: 2 vs 1. The 1 is broadcasted to 2.\n",
            "- The first dimension: 3 vs nothing (implicitly 1). The 1 is broadcasted to 3.\n",
            "\n",
            "So, the (1,4) tensor is effectively \"expanded\" to (3, 2, 4) by replicating its rows and then the element-wise addition is performed.\n",
            "\n",
            "Broadcasting simplifies code and improves performance, but it's crucial to understand how it works to avoid unexpected results.  If shapes are not compatible for broadcasting, TensorFlow will raise an error.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Create a random tensor of shape (4, 6)\n",
        "tensor = tf.random.normal(shape=(4, 6))\n",
        "\n",
        "# 2. Find its rank and shape using TensorFlow functions.\n",
        "rank_before = tf.rank(tensor)\n",
        "shape_before = tf.shape(tensor)\n",
        "\n",
        "print(\"Original Tensor:\")\n",
        "print(tensor)\n",
        "print(\"Rank before reshaping:\", rank_before.numpy())\n",
        "print(\"Shape before reshaping:\", shape_before.numpy())\n",
        "\n",
        "# 3. Reshape it into (2, 3, 4) and transpose it to (3, 2, 4).\n",
        "reshaped_tensor = tf.reshape(tensor, (2, 3, 4))\n",
        "transposed_tensor = tf.transpose(reshaped_tensor, perm=[1, 0, 2])  # Note the permutation for transpose\n",
        "\n",
        "rank_after_reshape = tf.rank(reshaped_tensor)\n",
        "shape_after_reshape = tf.shape(reshaped_tensor)\n",
        "\n",
        "rank_after_transpose = tf.rank(transposed_tensor)\n",
        "shape_after_transpose = tf.shape(transposed_tensor)\n",
        "\n",
        "\n",
        "print(\"\\nReshaped Tensor:\")\n",
        "print(reshaped_tensor)\n",
        "print(\"Rank after reshaping:\", rank_after_reshape.numpy())\n",
        "print(\"Shape after reshaping:\", shape_after_reshape.numpy())\n",
        "\n",
        "print(\"\\nTransposed Tensor:\")\n",
        "print(transposed_tensor)\n",
        "print(\"Rank after transposing:\", rank_after_transpose.numpy())\n",
        "print(\"Shape after transposing:\", shape_after_transpose.numpy())\n",
        "\n",
        "\n",
        "\n",
        "# 4. Broadcast a smaller tensor (1, 4) to match the larger tensor and add them.\n",
        "smaller_tensor = tf.random.normal(shape=(1, 4))\n",
        "\n",
        "# Broadcasting happens implicitly during the addition.  No explicit broadcast needed.\n",
        "broadcasted_sum = transposed_tensor + smaller_tensor\n",
        "\n",
        "print(\"\\nResult after broadcasting and addition:\")\n",
        "print(broadcasted_sum)\n",
        "\n",
        "\n",
        "# 5. Explain how broadcasting works in TensorFlow.\n",
        "print(\"\\nExplanation of Broadcasting:\")\n",
        "print(\"\"\"\n",
        "Broadcasting in TensorFlow (and NumPy) allows arithmetic operations between tensors of different shapes,\n",
        "as long as certain compatibility rules are met.  It avoids explicit data replication, making operations\n",
        "more memory and computationally efficient.\n",
        "\n",
        "In this example, 'smaller_tensor' with shape (1, 4) is added to 'transposed_tensor' with shape (3, 2, 4).\n",
        "Broadcasting works as follows:\n",
        "\n",
        "1. **Dimension Alignment:** TensorFlow compares the shapes of the two tensors dimension by dimension, starting from the trailing dimensions (rightmost).\n",
        "\n",
        "2. **Compatibility Rules:** Two dimensions are compatible if:\n",
        "   a) They are equal, or\n",
        "   b) One of them is 1.\n",
        "\n",
        "3. **Expansion:**  If a dimension in one tensor is 1, TensorFlow \"stretches\" or \"copies\" that dimension to match the corresponding dimension in the other tensor.  This is the \"broadcasting\" part.\n",
        "\n",
        "In our case:\n",
        "- (3, 2, 4) and (1, 4)\n",
        "\n",
        "- The last dimension (4) matches.\n",
        "- The second to last dimension: 2 vs 1. The 1 is broadcasted to 2.\n",
        "- The first dimension: 3 vs nothing (implicitly 1). The 1 is broadcasted to 3.\n",
        "\n",
        "So, the (1,4) tensor is effectively \"expanded\" to (3, 2, 4) by replicating its rows and then the element-wise addition is performed.\n",
        "\n",
        "Broadcasting simplifies code and improves performance, but it's crucial to understand how it works to avoid unexpected results.  If shapes are not compatible for broadcasting, TensorFlow will raise an error.\n",
        "\"\"\")"
      ]
    }
  ]
}